\newpage
\section*{Problems Classes}
\addcontentsline{toc}{section}{\protect\numberline{}Problems Classes}
    \subsection*{07-Feb-2020}
    \addcontentsline{toc}{subsection}{\protect\numberline{}07-Feb-2020}
        \paragraph{11}
        \textit{$n = 100$ random samples of water from a fresh water lake
        were taken and the calcium concentration (given in milligrams per
        litre) measured. A 95\% confidence interval on the mean calcium
        concentration $\Theta$ is $[0.49, 0.82]$, where the standard
        deviation is assumed to be known}

        \textit{(a) Consider the following statement: There is a 95\% chance
        that $\Theta$ is between 0.49 and 0.82. Is this statement correct?
        Explain your answer}

            No, the statement is not correct, as the confidence interval is
            conditioned on $\theta$, so we are making a probability
            statement about the \textit{data}, not the parameter!

            \[
                \bb{P}(\bar X_n - 1.96 \frac{\sigma}{\sqrt{n}} \leq \theta
                \leq \bar X_n + 1.96 \frac{\sigma}{\sqrt{n}}) = 0.95
            \]

            From the information we know, the probability $\bb{P}(0.49 \leq
            \Theta \leq 0.82)$ could be anywhere between 0 and 1, and
            actually depends on the prior.

            Also, $\bb{P}(0.49 \leq \theta \leq 0.82 | \theta)$ will be
            either 0 or 1 depending on $\theta$.

        \textit{(b) The process of taking $n = 100$ random samples of water
        from the lake and computing a 95\% confidence interval on $\Theta$
        is repeated 1000 times, each time obtaining a slightly different
        interval due to randomness across the different samples. One of
        these intervals is $[0.49, 0.82]$. The true value $\theta$ of
        $\Theta$ is then revealed. Someone claims that precisely 950 of the
        1000 95\% confidence intervals computed earlier should contain
        $\theta$. Is this correct? Explain your answer.}

            This is \textit{not} correct, although we could say that it's
            ``almost'' correct. We need to remember that, ultimately, the
            number of intervals that will contain $\theta, Y,$ is a random
            variable, and is not known in advance. $Y$ may be thought of as
            a sum of Bernoulli variables:

            \[
                Y = \sum_{i=1}^{1000} I_{A_i}, \quad A_i =\{\overline
                X_{n,i} - 1.96 \frac{\sigma}{\sqrt{n}} \leq \Theta \leq ...
                + 1.96 ... \}
            \]

            If conditional on $\Theta = \theta, Y$ will still not have a
            fixed value! Recall:

            \[
                I_{A_i}(\omega) := \begin{cases}1, & \omega \in A_i \\ 0, &
                \omega \not \in A_i\end{cases}
            \]

            Note that $\bb{P}(A_i | \theta) = 0.95$.

        \textit{(c) Let $Y$ denote the number of 1000 95\% confidence
        intervals that contain $\Theta$. Identify the unconditional
        distribution of $Y$, and compute its unconditional mean and standard
        deviation. Finally, identify two numbers $a, b$ such that
        (approximately) $\bb{P}(a \leq Y \leq b) = 0.95$.}

            By (b), we have:

            \[
                Y | \theta \sim \text{Bin}(1000, 0.95)
            \]

            Because this conditional distribution doesn't depend on
            $\Theta$, by the partition theorem, we have:

            \[
                p(y) = \int_0^1 p(y | \theta) f(\theta) d\theta =
                p(y | \theta)
            \]

            Note that $p(y | \theta)$ doesn't depend on $\theta$, so we have

            \[
                p(y) = p(y | \theta) \int_0^1 f(\theta) d\theta
            \]

            So, $Y \sim \text{Bin}(1000, 0.95)$.

            Note that $\E(Y) = 1000 \cdot 0.95 = 950, \var(Y) = 1000 \cdot
            0.95 \cdot 0.05 = 6.89^2$.

            Since we have a large $n$, we may approximate the binomial
            distribution by the normal distribution, so approximately:

            \[
                Y \sim \mathcal{N}(950, 6.89^2)
            \]

            Therefore, a 95\% interval on $Y$ can be calculated:

            \[
                \bb{P}(950 - 1.96 \cdot 6.89 \leq Y \leq 950 + 1.96 \cdot
                6.89) = 0.95
            \]
            \[
                [936.5, 963.5]
            \]

        \paragraph{22}
        \textit{Consider again the conditions of exercise 21, and assume the
        same prior distribution of $\Theta$. Suppose now, however, that six
        observations are selected at random from the uniform distribution
        $[\theta - 1/2, \theta + 1/2]$, and their values are $11.0, 11.5,
        11.7, 11.1, 11.4, 10.3$. Determine the posterior distribution}.

            We have $X_i | \theta \sim \mathcal{U}(\theta - 1/2, \theta +
            1/2)$, and the prior $\Theta \sim \mathcal{U}(10, 20)$.

            \[
                f(\theta | x_1, ..., x_n) \propind{\theta} f(\theta)
                \prod_{i=1}^n f(x_i | \theta)
            \]

            where $f(\theta) \propind{\theta} \begin{cases}1, & \theta \in
            [10, 20] \\ 0, &\text{otherwise}\end{cases}$.

            \[
                \implies f(x_i | \theta) = \begin{cases}1, & x_i \in [\theta
                - 1/2, \theta + 1/2] \\ 0, & \text{otherwise}\end{cases}
            \]

            $x_i \in [\theta - 1/2, \theta + 1/2] \iff \theta \in [x_i -
            1/2, x_i + 1/2]$, so we have:

            \[
                f(\theta | x_1, ..., x_n) \propind{\theta} \begin{cases}1, &
                \theta \in [10, 20] \land \theta \in [x_i - 1/2, x_i + 1/2]
                \,\fa i = 1, ..., n \\ 0, & \text{otherwise}\end{cases}
            \]

            The first condition of this piecewise function holds $\iff
            \theta \in [10, 20] \cap \bigcap\limits_{i=1}^n [x_i - 1/2, x_i
            + 1/2] \iff \theta \in [10] \cap [\max\limits_{i=1}^n x_i - 1/2,
            \min\limits_{i=1}^n x_i + 1/2]$. Let $x^* = \max\limits_{i=1}^n
            x_i, x_* = \min\limits_{i = 1}^n x_i$ So we have:

            \[
                \Theta | x_1, ..., x_n \sim \mathcal{U}(x^* - 1/2, x_* +
                1/2) = \mathcal{U}(11.2, 11.4)
            \]

        \paragraph{33}
        \textit{Let $\theta$ denote the average number of defects per 100
        feet of a certain type of magnetic tape. Suppose that the value of
        $\theta$ is unknown and that the prior distribution of $\Theta$ is
        the gamma distribution with parameters $\alpha_0 = 2, \beta_0 = 10$.
        When a 1200-foot roll of this tape is inspected, exactly four
        defects are found. Determine the posterior distribution of
        $\Theta$.}
            \begin{relq}
                Consider exercise 31 first!
            \end{relq}

            We could pretend that instead of one 1200 foot roll, we instead
            have 12 one foot rolls, with a total of $\sum\limits_{i=1}^{12}
            x_i = 4$ defects. Therefore, by exercise 31, the posterior
            distribution is:

            \[
                \text{Gamma}\left(\alpha_0 + \sum\limits_{i=1}^{n} x_i,
                \beta_0 + n\right) = \text{Gamma}(3 + 4, 1 + 12) =
                \text{Gamma}(7, 13)
            \]

    \subsection*{21-Feb-2020}
    \addcontentsline{toc}{subsection}{\protect\numberline{}21-Feb-2020}
        \paragraph{38}
        \textit{In a clincal trial, let the probability of successful
        outcome $\Theta$ have a prior distribution that is the uniform
        distribution on the interval $[0, 1]$, which is also the beta
        distribution with parameters $1$ and $1$. Suppose that the first
        patient has a successful outcome. Find the Bayes estimate of
        $\Theta$ under squared error loss.}

            We have the prior $\Theta \sim \mathcal{U}(0, 1) =
            \text{Beta}(1, 1)$, and the sampling $X_i | \theta \sim
            \text{Bernoulli}(\theta)$. We have $n = 1, x_1 = 1$. From
            lectures, we have $\widehat \theta = \E(\Theta | x_1, ...,
            x_n)$, so the question is actually just asking us to find the
            posterior expectation under squared error loss.

            By conjugacy, we have $\Theta | x_1 \sim \text{Beta}(1 + 1, 1 +
            1 - 1) = \text{Beta}(2, 1)$, since $\alpha_1 = \alpha_0 + x_1,
            \beta_1 = \beta_0 + n - x_1$.

            By the properties of the beta distribution, we have:

            \[
                \E(\Theta | x_1) = \frac{\alpha_1}{\alpha_1 + \beta_1} =
                \frac{2}{3}
            \]

        \paragraph{41}
        \textit{A random sample of size $n$ is taken from the Bernoulli
        distribution with parameter $\theta$, which is unknown. The prior
        distribution for $\theta$ is a beta distribution for which the mean
        is $\mu_0$. Show that the mean of the posterior expectation of
        $\Theta$ will be a weighted average of the form $\gamma_n \overline
        x_n + (1 - \gamma_n)\mu_0$, and show that $\gamma_n \to 1$ as $n \to
        \infty$.}

            We have sampling $X_i | \theta \sim \text{Bernoulli}(\theta)$,
            with prior $\Theta \sim \text{Beta}(\alpha_0, \beta_0)$.
            Therefore, $\E(\Theta) = \frac{\alpha_0}{\alpha_0 + \beta_0} =
            \mu_0$.

            By conjugacy, we have:

            \begin{align*}
                \E(\Theta | x_1, ..., x_n) & = \frac{\alpha_n}{\alpha_n +
                    \beta_n} \\
                & = \frac{\alpha_0 + \sum\limits_{i=1}^n x_i} {\alpha_0 +
                    \beta_0 + n} \\
                & = \frac{\alpha_0}{\alpha_0 + \beta_0 + n} + \frac{n}
                    {\alpha_0 + \beta_0 + n}\overline x_n
            \end{align*}

            Is the denominator $\alpha_0 + \beta_0 + n$ equal to $\gamma_n$?
            Yes, if the first fraction $\frac{\alpha_0}{\alpha_0 + \beta_0 +
            n}$ is equal to $(1 - \gamma_n)\mu_0$.

            We can check this:

            \begin{align*}
                (1 - \gamma_n)\mu_0 & = \frac{\alpha_0 + \beta_0}{\alpha_0 +
                    \beta_0 + n} \cdot \frac{\alpha_0}{\alpha_0 + \beta_0}\\
                & = \frac{\alpha_0}{\alpha_0 + \beta_0 + n}
            \end{align*}

            So yes, we've demonstrated that this is a weighted average.

        \paragraph{49}
        \textit{It is not known what proportion $\Theta$ of the purchases of
        a certain brand of breakfast cereal are made by women, and what
        proportion are made by men. In a random sample of 70 purchases of
        this cereal, it was found that 58 were made by women and 12 were
        made by men.}

        \textit{(a) Find the maximum likelihood estimator of $\Theta$.}

            We are doing Bernoulli sampling. Let $X_i$ equal $1$ if female,
            $0$ if male. Then $X_i | \theta \sim \text{Bernoulli}(\theta)$.

            For this sampling, the MLE is therefore $\widehat \theta =
            \frac{\sum\limits_{i=1}^n x_i}{n} \approx 0.83$.

        \textit{(b) Now suppose that it is additionally known that
        $\frac{1}{2} \leq \Theta \leq \frac{2}{3}$. What is now the maximum
        likelihood likelihood estimator of $\Theta$? (Hint: When is
        $L'(\theta) > 0$?)}

            Let's answer the hint first, since if we want to maximise this
            function, we just need to maximise the log likelihood.

            \[
                L(\theta) = y\log\theta + (n - y)\log(1 - \theta), \quad y =
                \sum_{i=1}^n x_i
            \]
            \begin{align*}
                & L'(\theta) > 0 \\
                \iff & \frac{y}{\theta} - \frac{n - y}{1 - \theta} > 0 \\
                \iff & (1 - \theta)y - \theta(n - y) > 0 \\
                \iff & y - n\theta > 0 \\
                \iff & \theta < \frac{y}{n} = \frac{58}{70} \approx 0.83
            \end{align*}

            So the maximum of the log likelihood $L(\theta)$ over
            $\left[\frac{1}{2}, \frac{2}{3}\right]$ is achieved at $\widehat
            \theta = \frac{2}{3}$ (as the gradient is always positive, so
            the function is strictly increasing).

        \paragraph{50}
        \textit{Suppose that $X_1, ..., X_n$ form a random sample from a
        Poisson distribution for which the mean $\theta$ is unknown, with
        $\theta > 0$.}

        \textit{(a) Determine the maximum likelihood estimate of $\Theta$,
        assuming that at least one of the oserved values is different from
        $0$.}

            We have sampling given by $X_i | \theta \sim \text{Po}(\theta)$,
            so $p(x_i | \theta) = \theta^{x_i} \frac{e^{-\theta}}{x_i!}$,
            and $\log{p}(x_i | \theta) = x_i\log{\theta}- \theta -
            \log(x_i!)$. So:

            \begin{align*}
                L(\theta) & = \log p(x_1, ..., x_n | \theta) \\
                & = \log\prod_{i=1}^n p(x_i | \theta) \\
                & = \sum_{i=1}^n \log p(x_i | \theta) \\
                & = \sum_{i=1}^n (x_i\log\theta - \theta - \log(x_i!)) \\
                & = \log\theta \sum_{i=1}^n x_i - n\theta - D
            \end{align*}

            where $D$ is a constant that doesn't depend on $\theta$.

            Therefore:

            \[
                L'(\theta) = \frac{\sum\limits_{i=1}^n x_i}{\theta} - n =
                \frac{n\overline x_n}{\theta} - n
            \]
            \[
                L''(\theta) = -\frac{n\overline x_n}{\theta^2}
            \]

            So $L'(\widehat \theta) = 0$ when $\widehat \theta = \overline
            x_n$, provided $x_n > 0$ (so when at least one $x_i > 0$).
            Further $L''(\overline \theta) < 0$, so it is a maximum.

        \textit{(b) Show that the maximum likelihood estimate of $\Theta$
        does not exist if every observed value is $0$.}

            If all $x_i = 0$, then $\overline x_n = 0$, and $L'(\theta) = -n
            < 0$. This achieves a maximum near $\theta = 0$, but for $\theta
            = 0, \text{Po}(\theta)$ is not a valid distribution, as
            $\widehat \theta$ does not exist in this case!

    \subsection*{06-Mar-2020}
    \addcontentsline{toc}{subsection}{\protect\numberline{}06-Mar-2020}
        \paragraph{65}
        \textit{Suppose that $X_1, ..., X_n$ form a random sample from the
        normal distribution with known mean $\mu$ and known variance
        $\sigma^2$. Let $S_n^2 := \sum\limits_{i=1}^n (X_i - \overline
        X_n)^2$. Determine the smallest value of $n$ for which the following
        relation is satisfied:}

        \[
            \bb{P}\left(\frac{S_n^2}{n\sigma^2} \leq 1.5\right) \geq 0.95
        \]

            We know from lectures that:

            \[
                Y_{n-1} := \frac{S_n^2}{\sigma^2} \sim \chi^2(n - 1)
            \]

            Therefore, we need to find an $n$ such that:

            \[
                \bb{P}\left(\frac{S_n^2}{\sigma^2} \leq 1.5n\right) =
                \bb{P}(Y_{n-1} \leq 1.5n) \geq 0.95
            \]

            Looking at the table for the Chi-Square distribution, with $n =
            20$, we see that $\bb{P}(Y_{n-1} \leq 30.14) = 0.95$, so we must
            have $\bb{P}(Y_{n-1} \leq 1.5n = 30) < 0.95$. Therefore, $n =
            21$ is the lowest value of $n$ satisfying the inequality.

        \paragraph{68}
        \textit{Suppose that the five random variables $X_1, ..., X_5$ are
        IID and that each has the standard normal distribution. Determine a
        constant $c$ such that the random variable given by:}

        \[
            \frac{c(X_1 + X_2)}{(X_3^2 + X_4^2 + X_5^2)^{1/2}}
        \]

        \textit{will have a $t$ distribution.}

            If $X \sim \mathcal{N}(0, 1)$, and $Y \sim \chi^2(n)$ are
            independent, then $\frac{X}{\sqrt{Y}{n}} \sim t(n)$. Now note the
            following:

            \[
                \frac{1}{\sqrt{2}} (X_1 + X_2) \sim \mathcal{N}(0, 1)
            \]
            \[
                (X_3^2 + X_4^2 + X_5^2) \sim \chi^2(3)
            \]

            Therefore, we have:

            \[
                \frac{1/\sqrt{2} \cdot (X_1 + X_2)}{\sqrt{1/3 \cdot (X_3^2 +
                X_4^2 + X_5^2)}} \sim t(3) \implies c = \sqrt{\frac{3}{2}}
            \]

        \paragraph{73}
        \textit{Show that, for normal sampling with unknown mean $\mu$ and
        unknown variance $\sigma^2$, for large $n$, we approximately have
        that:}

        \[
            \bb{P}\left(\left.n - 1 - z_{\alpha/2}\sqrt{2(n - 1)} \leq
            \frac{S_n^2}{\sigma^2} \leq n - 1 + z_{\alpha/2}\sqrt{2(n - 1)}
            \right|\mu, \sigma^2\right) = 1 - \alpha
        \]

        \textit{where $z_{\alpha/2} := \Phi^{-1}(1 - \alpha/2)$ as usual.}

            For large $n$, by the CLT, we have:

            \[
                \left.\frac{S_n^2}{\sigma^2}\right|\mu, \sigma^2 \sim
                \chi^2(n -1) \approx \mathcal{N}(n - 1, 2(n - 1))
            \]

            since $\E(y) = n, \var(Y) = 2n$ for $Y \sim \chi^2(n)$.

            \begin{align*}
                & \bb{P}\left(\left.n - 1 - z_{\alpha/2}\sqrt{2(n - 1)} \leq
                    \frac{S_n^2}{\sigma^2} \leq n - 1 + z_{\alpha/2}
                    \sqrt{2(n - 1)} \right|\mu, \sigma^2\right) = 1 - \alpha
                    \\
                \iff & \bb{P}\left(\left.-z_{\alpha/2} \leq \frac{1}{\sqrt{2
                    (n-1)}}\left(\frac{S_n^2}{\sigma^2} - (n - 1)\right)
                    \leq z_{\alpha/2}\right|\mu, \sigma^2\right) = 1 -
                    \alpha
            \end{align*}

            Now note that we have the following approximate result:

            \[
                \frac{1}{\sqrt{2 (n-1)}}\left(\frac{S_n^2}{\sigma^2} - (n -
                1)\right) \sim \mathcal{N}(0, 1)
            \]

            The second statement clearly then holds, since this is a
            well-known result for the normal distribution. Therefore, the
            proposition holds.

        \paragraph{74}
        \textit{(a) Show that, for any sequence of numbers $x_1, ..., x_n$
        we have that:}

        \[
            \sum_{i=1}^n (x_i - \overline x_n)^2 = \sum_{i=1}^n x_i^2 -
            n\overline x_n^2
        \]

        \textit{where $\overline x_n := \frac{1}{n} \sum\limits_{i=1}^n x_i$
        as usual.}

            \begin{align*}
                \sum_{i=1}^n (x_i - \overline x_n)^2 & = \sum_{i=1}^n x_i^2
                    - 2\overline x_n \sum_{i=1} x_i + n \overline x_n^2 \\
                & = \sum_{i=1}^n x_i^2 - 2 n\overline x_n^2 + n \overline
                    x_n^2 \\
                & = \sum_{i=1}^n x_i^2 - n\overline x_n^2
            \end{align*}

        \textit{(b) An article in cancer research tested the tumorigenesis
        of a drug. Rats were randomly selected from litter and given the
        drug. The times of tumour appearance were recorded as follows:}

        \begin{center}
            \textit{(snipped for space, see the actual problem sheet)}
        \end{center}

        \textit{It can be verified that the number of observations is 41,
        the sum of the data is 3640, and the sum of squares of the data is
        333392. Calculate $s_n^2$ for the above data.}

            Using the above fact, we have:

            \begin{align*}
                s_n^2 & = \sum_{i=1}^n (x_i - \overline x_n)^2 \\
                & = \sum_{i=1}^n x_i^2 - n\overline x_n^2 \\
                & = 333392 - 41 \left(\frac{3640}{31}\right)^2 \\
                & = 10231.02
            \end{align*}

        \textit{(c) For the same cancer data, use the normal approximation
        to the $\chi^2$ distribution to find the observed value of a 95\%
        confidence interval for the standard deviation of time until a
        tumour appearance. Explicitly state any assumptions that you make,
        and explain why a normal approximation is justified for this case.}

            Since $n$ is large, we know the following:

            \[
                \bb{P}\left(\left.n - 1 - z_{\alpha/2} \sqrt{2(n-1)} \leq
                \frac{S_n^2}{\sigma^2} \leq n - 1 +
                z_{\alpha/2}\sqrt{2(n-1)} \right|\mu, \sigma^2\right) = 1 -
                \alpha
            \]
            \[
                \implies \bb{P}\left(\left.\sqrt{\frac{S_n^2}{n - 1 +
                z_{\alpha/2}\sqrt{2(n-1)}}} \leq \sigma \leq
                \sqrt{\frac{S_n^2}{n - 1 - z_{\alpha/2}\sqrt{2(n-1)}}}
                \right|\mu, \sigma^2\right)
            \]

            Therefore, we have an approximate confidence interval on
            $\sigma$. For our data, with $1 - \alpha = 0.95$, we find:

            \[
                \left[\sqrt{\frac{S_n^2}{n - 1 +
                z_{\alpha/2}\sqrt{2(n-1)}}},
                \sqrt{\frac{S_n^2}{n - 1 - z_{\alpha/2}\sqrt{2(n-1)}}}\,\,
                \right] = [13.34, 21.34]
            \]
